import os

import google.generativeai as genai
import streamlit as st
import torch
from dotenv import dotenv_values

from modules.input_to_output import input_to_output
from modules.llm.res.output_miss_to_response import output_miss_to_response
from modules.llm.res.output_to_response_mct import output_to_response_mct
from modules.llm.res.output_to_response_move import output_to_response_move

base_dir = os.path.dirname(__file__)
config_dir = os.path.join(base_dir, "./.streamlit/secrets.toml")

config = dotenv_values(config_dir)

GOOGLE_API_KEY = config.get("GOOGLE_API_KEY")

genai.configure(api_key=GOOGLE_API_KEY)

model = genai.GenerativeModel("gemini-1.5-flash")

st.set_page_config(page_title="ğŸŠì°¸ì‹ í•œ ì œì£¼ ë§›ì§‘!")

# Replicate Credentials
with st.sidebar:
    st.title("ğŸŠíš¨ìœ¨ì ì¸! ì œì£¼ ë™ì„ ")

    st.write("")

    st.subheader("ê°€ì¥ ì¢‹ì€ ê°ˆíŒ¡ ì°¾ì•„ì¤„ê²Œë§ˆì”¸.")

    st.write("")

    # ì‚¬ìš©ì„¤ëª…ì„œ ë‚´ìš© ì¶”ê°€
    st.sidebar.markdown(
        """
    ** ì‚¬ìš©ì„¤ëª…ì„œ **
    1. MCT ë°ì´í„° ê´€ë ¨í•´ì„œ ì§ˆë¬¸ì„ í•˜ê³  ì‹¶ìœ¼ë©´ 'ë§›ì§‘ ì¶”ì²œ'ì„ ì„ íƒ, ë™ì„  ê´€ë ¨í•´ì„œ ì§ˆë¬¸ì„ í•˜ë ¤ë©´ 'ë™ì„  ì¶”ì²œ'ì„ ì„ íƒí•´ì£¼ì„¸ìš”.
    2. 'ë™ì„  ì¶”ì²œ'ì€ ì‹œì‘í•˜ëŠ” ì¥ì†Œ, ë“¤ë¦¬ê³  ì‹¶ì€ ì¥ì†Œë“¤, ë°©ë¬¸í•˜ëŠ” ì›”,ìš”ì¼,ì‹œê°„ëŒ€ë¥¼ ì…ë ¥í•´ì£¼ë©´ ë” ìì„¸í•˜ê²Œ ì°¾ì•„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    )

    st.write("")

    st.markdown(
        """
        <style>
        .stRadio > label {
            display: none;
        }
        .stRadio > div {
            margin-top: -20px;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    move_choice = st.radio("", ("ë§›ì§‘ ì¶”ì²œ", "ë™ì„  ì¶”ì²œ"))

st.title("ì‚¼ì¶˜! ë°¥ ë¨¹ì–¸? ì¶”ì²œ ì¢€ ë°›ê³  ê°€ê²Œ!")
st.subheader("ë¬´ì‚¬ ë°¥ì„ ì•ˆ ë¨¹ì–¸! ì˜ ê°‘ì„œì–‘.")

st.write("")

st.write("#ë§›ì§‘ #ë™ì„  #ê´€ê´‘ì§€ #ì‡¼í•‘ #ìˆ™ë°• #ì œì£¼ ğŸ¤¤")

st.write("")

image_path = "https://i.ibb.co/M2vQKFs/202101003982-500.jpg"
image_html = f"""
<div style="display: flex; justify-content: center;">
    <img src="{image_path}" alt="centered image" width="50%">
</div>
"""
st.markdown(image_html, unsafe_allow_html=True)

st.write("")

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [
        {"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}
    ]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])


def clear_chat_history():
    st.session_state.messages = [
        {"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}
    ]


st.sidebar.button("Clear Chat History", on_click=clear_chat_history)


# RAG

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Device is {device}.")


# User-provided prompt
if prompt := st.chat_input():  # (disabled=not replicate_api):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)


if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            placeholder = st.empty()
            if move_choice == "ë§›ì§‘ ì¶”ì²œ":
                move_choice = "MCT"
            elif move_choice == "ë™ì„  ì¶”ì²œ":
                move_choice = "MOVE"

            source_output = input_to_output(prompt, move_choice, model)

            if source_output == "-1":
                response_text = output_miss_to_response(model)
            else:
                if move_choice == "MCT":
                    response_text = output_to_response_mct(source_output, model)
                elif move_choice == "MOVE":
                    response_text = output_to_response_move(source_output, model)
            placeholder.markdown(response_text)  # ìµœì¢… ì‘ë‹µì„ ì¶œë ¥

    # ì„¸ì…˜ ìƒíƒœì— ë©”ì‹œì§€ ì¶”ê°€
    message = {"role": "assistant", "content": response_text}
    st.session_state.messages.append(message)
